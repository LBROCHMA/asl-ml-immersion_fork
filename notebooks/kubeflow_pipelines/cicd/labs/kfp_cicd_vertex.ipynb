{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD for a Kubeflow pipeline on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to create a custom Cloud Build builder to pilote Vertex AI Pipelines\n",
    "1. Learn how to write a Cloud Build config file to build and push all the artifacts for a KFP\n",
    "1. Learn how to setup a Cloud Build GitHub trigger a new run of the Kubeflow PIpeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will walk through authoring of a **Cloud Build** CI/CD workflow that automatically builds, deploys, and runs a Kubeflow pipeline on Vertex AI. You will also integrate your workflow with **GitHub** by setting up a trigger that starts the  workflow when a new tag is applied to the **GitHub** repo hosting the pipeline's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = \"us-central1\"\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the artifact store exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-02-3d939c98ad80-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the KFP CLI builder for Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, write a docker file that\n",
    "* Uses `gcr.io/deeplearning-platform-release/base-cpu` as base image\n",
    "* Install the python packages `kfp` with version `1.6.6 ` and `google-cloud-aiplatform` with version `1.3.0`\n",
    "* Starts `/bin/bash` as entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kfp-cli_vertex/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile kfp-cli_vertex/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install kfp==1.6.6\n",
    "RUN pip install google-cloud-aiplatform==1.3.0\n",
    "ENTRYPOINT [\"/bin/bash\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image and push it to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex:latest'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KFP_CLI_IMAGE_NAME = \"kfp-cli-vertex\"\n",
    "KFP_CLI_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{KFP_CLI_IMAGE_NAME}:latest\"\n",
    "KFP_CLI_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, use `gcloud builds` to build the `kfp-cli-vertex` Docker image and push it to the project gcr.io registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 1.1 KiB before compression.\n",
      "Uploading tarball of [kfp-cli_vertex] to [gs://qwiklabs-asl-02-3d939c98ad80_cloudbuild/source/1687335803.649876-eda8654d7da645e5b57190653cbf117d.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-02-3d939c98ad80/locations/global/builds/beaba677-5cf1-44f5-b695-9384324c0b79].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/beaba677-5cf1-44f5-b695-9384324c0b79?project=797777062396 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"beaba677-5cf1-44f5-b695-9384324c0b79\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-02-3d939c98ad80_cloudbuild/source/1687335803.649876-eda8654d7da645e5b57190653cbf117d.tgz#1687335803891342\n",
      "Copying gs://qwiklabs-asl-02-3d939c98ad80_cloudbuild/source/1687335803.649876-eda8654d7da645e5b57190653cbf117d.tgz#1687335803891342...\n",
      "/ [1 files][  925.0 B/  925.0 B]                                                \n",
      "Operation completed over 1 objects/925.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "56e0351b9876: Pulling fs layer\n",
      "4805448d2887: Pulling fs layer\n",
      "6010ee0be6fa: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "e2338d0c32f5: Pulling fs layer\n",
      "f44de40d5a81: Pulling fs layer\n",
      "076280f960a0: Pulling fs layer\n",
      "0735612d0a87: Pulling fs layer\n",
      "cd1fda9a7709: Pulling fs layer\n",
      "2cace1757ac5: Pulling fs layer\n",
      "9c90296fbff7: Pulling fs layer\n",
      "60fb050d3256: Pulling fs layer\n",
      "49eac6151c74: Pulling fs layer\n",
      "23ce0ff19339: Pulling fs layer\n",
      "2e6deadecd70: Pulling fs layer\n",
      "74755647fd11: Pulling fs layer\n",
      "bd19886240a6: Pulling fs layer\n",
      "057629cf5e3d: Pulling fs layer\n",
      "ceeeb7fc204d: Pulling fs layer\n",
      "66bc44577f21: Pulling fs layer\n",
      "637548efc9d8: Pulling fs layer\n",
      "4b55ed3e4d80: Pulling fs layer\n",
      "0ec5ac20ec04: Pulling fs layer\n",
      "e2338d0c32f5: Waiting\n",
      "f44de40d5a81: Waiting\n",
      "076280f960a0: Waiting\n",
      "0735612d0a87: Waiting\n",
      "cd1fda9a7709: Waiting\n",
      "2cace1757ac5: Waiting\n",
      "9c90296fbff7: Waiting\n",
      "60fb050d3256: Waiting\n",
      "49eac6151c74: Waiting\n",
      "23ce0ff19339: Waiting\n",
      "2e6deadecd70: Waiting\n",
      "74755647fd11: Waiting\n",
      "bd19886240a6: Waiting\n",
      "057629cf5e3d: Waiting\n",
      "ceeeb7fc204d: Waiting\n",
      "66bc44577f21: Waiting\n",
      "637548efc9d8: Waiting\n",
      "4b55ed3e4d80: Waiting\n",
      "0ec5ac20ec04: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "6010ee0be6fa: Verifying Checksum\n",
      "6010ee0be6fa: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "56e0351b9876: Verifying Checksum\n",
      "56e0351b9876: Download complete\n",
      "4805448d2887: Verifying Checksum\n",
      "4805448d2887: Download complete\n",
      "076280f960a0: Download complete\n",
      "0735612d0a87: Verifying Checksum\n",
      "0735612d0a87: Download complete\n",
      "cd1fda9a7709: Verifying Checksum\n",
      "cd1fda9a7709: Download complete\n",
      "2cace1757ac5: Verifying Checksum\n",
      "2cace1757ac5: Download complete\n",
      "56e0351b9876: Pull complete\n",
      "9c90296fbff7: Verifying Checksum\n",
      "9c90296fbff7: Download complete\n",
      "60fb050d3256: Verifying Checksum\n",
      "60fb050d3256: Download complete\n",
      "49eac6151c74: Verifying Checksum\n",
      "49eac6151c74: Download complete\n",
      "23ce0ff19339: Verifying Checksum\n",
      "23ce0ff19339: Download complete\n",
      "2e6deadecd70: Verifying Checksum\n",
      "2e6deadecd70: Download complete\n",
      "74755647fd11: Verifying Checksum\n",
      "74755647fd11: Download complete\n",
      "bd19886240a6: Verifying Checksum\n",
      "bd19886240a6: Download complete\n",
      "057629cf5e3d: Verifying Checksum\n",
      "057629cf5e3d: Download complete\n",
      "ceeeb7fc204d: Download complete\n",
      "66bc44577f21: Verifying Checksum\n",
      "66bc44577f21: Download complete\n",
      "637548efc9d8: Verifying Checksum\n",
      "637548efc9d8: Download complete\n",
      "e2338d0c32f5: Verifying Checksum\n",
      "e2338d0c32f5: Download complete\n",
      "f44de40d5a81: Verifying Checksum\n",
      "f44de40d5a81: Download complete\n",
      "4805448d2887: Pull complete\n",
      "6010ee0be6fa: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "0ec5ac20ec04: Verifying Checksum\n",
      "0ec5ac20ec04: Download complete\n",
      "4b55ed3e4d80: Verifying Checksum\n",
      "4b55ed3e4d80: Download complete\n",
      "e2338d0c32f5: Pull complete\n",
      "f44de40d5a81: Pull complete\n",
      "076280f960a0: Pull complete\n",
      "0735612d0a87: Pull complete\n",
      "cd1fda9a7709: Pull complete\n",
      "2cace1757ac5: Pull complete\n",
      "9c90296fbff7: Pull complete\n",
      "60fb050d3256: Pull complete\n",
      "49eac6151c74: Pull complete\n",
      "23ce0ff19339: Pull complete\n",
      "2e6deadecd70: Pull complete\n",
      "74755647fd11: Pull complete\n",
      "bd19886240a6: Pull complete\n",
      "057629cf5e3d: Pull complete\n",
      "ceeeb7fc204d: Pull complete\n",
      "66bc44577f21: Pull complete\n",
      "637548efc9d8: Pull complete\n",
      "4b55ed3e4d80: Pull complete\n",
      "0ec5ac20ec04: Pull complete\n",
      "Digest: sha256:cac9f0c3e19cdac79a395ef2ea20db4e36b18e4135a66d02b84dec75f0a3995b\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 584d1bb363f5\n",
      "Step 2/4 : RUN pip install kfp==1.6.6\n",
      " ---> Running in e6f759411451\n",
      "Collecting kfp==1.6.6\n",
      "  Downloading kfp-1.6.6.tar.gz (224 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.8/224.8 kB 13.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting absl-py<=0.11,>=0.9 (from kfp==1.6.6)\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.8/127.8 kB 21.0 MB/s eta 0:00:00\n",
      "Collecting PyYAML<6,>=5.3 (from kfp==1.6.6)\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 636.6/636.6 kB 45.3 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage<2,>=1.20.0 (from kfp==1.6.6)\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 18.8 MB/s eta 0:00:00\n",
      "Collecting kubernetes<13,>=8.0.0 (from kfp==1.6.6)\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 68.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.6) (1.8.0)\n",
      "Collecting google-auth<2,>=1.6.1 (from kfp==1.6.6)\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 25.8 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0 (from kfp==1.6.6)\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 9.9 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<2,>=1.3.0 (from kfp==1.6.6)\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2 (from kfp==1.6.6)\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 9.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1 (from kfp==1.6.6)\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 9.8 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6 (from kfp==1.6.6)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting click<8,>=7.1.1 (from kfp==1.6.6)\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.8/82.8 kB 14.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.6) (1.2.14)\n",
      "Collecting strip-hints<1,>=0.1.8 (from kfp==1.6.6)\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3 (from kfp==1.6.6)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.8 (from kfp==1.6.6)\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting fire<1,>=0.3.1 (from kfp==1.6.6)\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 16.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.6) (3.20.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.6.6) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.6.6) (1.15.0)\n",
      "Collecting termcolor (from fire<1,>=0.3.1->kfp==1.6.6)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.6) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.6) (0.1.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.6) (1.34.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.6) (3.0.1)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.1->kfp==1.6.6)\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.6) (0.3.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.6) (67.7.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.6) (4.9)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.31.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (2.5.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.6) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.6) (0.18.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.6) (4.11.4)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.6) (1.26.16)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.6) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.6) (2.8.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.6) (1.5.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.6) (1.3.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.6.6) (0.40.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client<2,>=1.7.8->kfp==1.6.6) (1.59.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client<2,>=1.7.8->kfp==1.6.6) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.6.6) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.6) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.6) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.6) (4.6.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp==1.6.6) (3.2.2)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.6.6-py3-none-any.whl size=308891 sha256=029265b214847576b6c4d27d3bf345534994321df3affc86cfd377f0607e5b61\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/ec/07/d8a332e22c1098a45bdd0c54cb0dafd84bad0ca0e4f1b734ae\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=9eca97fc1cf573baad1b57bc59b84046984d1ba17c9359ea7010f12b57915cb4\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=59580a6884603f10841eef58e217596f3b59811def0f388d8dceeda0554b4fcc\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/0e/7b/ed385d69453b7b754834c01d83fa9f5708ba66b4f6ed5d6a35\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22284 sha256=25d8ffbeb5d25b052221a2373b68d273e2cd1809b45067c31678efac8aaf2f70\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: termcolor, tabulate, strip-hints, PyYAML, kfp-pipeline-spec, docstring-parser, cloudpickle, click, cachetools, absl-py, requests-toolbelt, kfp-server-api, google-auth, fire, kubernetes, jsonschema, google-cloud-storage, kfp\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.1\n",
      "    Uninstalling cloudpickle-2.2.1:\n",
      "      Successfully uninstalled cloudpickle-2.2.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.1\n",
      "    Uninstalling cachetools-5.3.1:\n",
      "      Successfully uninstalled cachetools-5.3.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.4.0\n",
      "    Uninstalling absl-py-1.4.0:\n",
      "      Successfully uninstalled absl-py-1.4.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.20.0\n",
      "    Uninstalling google-auth-2.20.0:\n",
      "      Successfully uninstalled google-auth-2.20.0\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 26.1.0\n",
      "    Uninstalling kubernetes-26.1.0:\n",
      "      Successfully uninstalled kubernetes-26.1.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.17.3\n",
      "    Uninstalling jsonschema-4.17.3:\n",
      "      Successfully uninstalled jsonschema-4.17.3\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.9.0\n",
      "    Uninstalling google-cloud-storage-2.9.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.9.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-auth-oauthlib 1.0.0 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "jupyterlab-server 2.23.0 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed PyYAML-5.4.1 absl-py-0.11.0 cachetools-4.2.4 click-7.1.2 cloudpickle-1.6.0 docstring-parser-0.15 fire-0.5.0 google-auth-1.35.0 google-cloud-storage-1.44.0 jsonschema-3.2.0 kfp-1.6.6 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-12.0.1 requests-toolbelt-0.10.1 strip-hints-0.1.10 tabulate-0.9.0 termcolor-2.3.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e6f759411451\n",
      " ---> 501b4577307a\n",
      "Step 3/4 : RUN pip install google-cloud-aiplatform==1.3.0\n",
      " ---> Running in 41942c90a1cf\n",
      "Collecting google-cloud-aiplatform==1.3.0\n",
      "  Downloading google_cloud_aiplatform-1.3.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 29.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (1.34.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (1.22.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (23.1)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.3.0) (1.44.0)\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0 (from google-cloud-aiplatform==1.3.0)\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.6/206.6 kB 25.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.59.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (3.20.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.35.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (2.5.0)\n",
      "Collecting packaging>=14.3 (from google-cloud-aiplatform==1.3.0)\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2.0.0dev,>=1.32.0->google-cloud-aiplatform==1.3.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.3.0) (3.0.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (0.3.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (67.7.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.3.0) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (2023.5.7)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.3.0) (0.5.0)\n",
      "Installing collected packages: packaging, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.11.1\n",
      "    Uninstalling google-cloud-bigquery-3.11.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.11.1\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.26.0\n",
      "    Uninstalling google-cloud-aiplatform-1.26.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.26.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.23.0 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.3.0 google-cloud-bigquery-2.34.4 packaging-21.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 41942c90a1cf\n",
      " ---> b1b1d2a46857\n",
      "Step 4/4 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in efca4a3ae67b\n",
      "Removing intermediate container efca4a3ae67b\n",
      " ---> 09fdee99c7eb\n",
      "Successfully built 09fdee99c7eb\n",
      "Successfully tagged gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex]\n",
      "704bdb10a9ce: Preparing\n",
      "10cb6c2ac919: Preparing\n",
      "6d8d8c0bc0c9: Preparing\n",
      "faf2c7881f6e: Preparing\n",
      "3a1279e9e180: Preparing\n",
      "74e7f2199704: Preparing\n",
      "137ec3936148: Preparing\n",
      "6dc2dc0ceda8: Preparing\n",
      "4e15aa2b0d93: Preparing\n",
      "6a9224648bb3: Preparing\n",
      "61d824b324cf: Preparing\n",
      "9abd177d62d4: Preparing\n",
      "5a26b52752f7: Preparing\n",
      "e1d00831a5f5: Preparing\n",
      "dd4157733cba: Preparing\n",
      "d62cb3f3ee3a: Preparing\n",
      "75612067b361: Preparing\n",
      "a957f356a963: Preparing\n",
      "240c1b858bf3: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "8338899cb13a: Preparing\n",
      "0313db964f3f: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "4fb340a451dc: Preparing\n",
      "e4ea0618e6f3: Preparing\n",
      "ec66d8cea54a: Preparing\n",
      "74e7f2199704: Waiting\n",
      "137ec3936148: Waiting\n",
      "6dc2dc0ceda8: Waiting\n",
      "4e15aa2b0d93: Waiting\n",
      "6a9224648bb3: Waiting\n",
      "61d824b324cf: Waiting\n",
      "9abd177d62d4: Waiting\n",
      "5a26b52752f7: Waiting\n",
      "e1d00831a5f5: Waiting\n",
      "dd4157733cba: Waiting\n",
      "d62cb3f3ee3a: Waiting\n",
      "75612067b361: Waiting\n",
      "a957f356a963: Waiting\n",
      "240c1b858bf3: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "8338899cb13a: Waiting\n",
      "0313db964f3f: Waiting\n",
      "4fb340a451dc: Waiting\n",
      "e4ea0618e6f3: Waiting\n",
      "ec66d8cea54a: Waiting\n",
      "6d8d8c0bc0c9: Mounted from deeplearning-platform-release/base-cpu\n",
      "faf2c7881f6e: Mounted from deeplearning-platform-release/base-cpu\n",
      "3a1279e9e180: Mounted from deeplearning-platform-release/base-cpu\n",
      "137ec3936148: Mounted from deeplearning-platform-release/base-cpu\n",
      "74e7f2199704: Mounted from deeplearning-platform-release/base-cpu\n",
      "6dc2dc0ceda8: Mounted from deeplearning-platform-release/base-cpu\n",
      "4e15aa2b0d93: Mounted from deeplearning-platform-release/base-cpu\n",
      "704bdb10a9ce: Pushed\n",
      "6a9224648bb3: Mounted from deeplearning-platform-release/base-cpu\n",
      "61d824b324cf: Mounted from deeplearning-platform-release/base-cpu\n",
      "9abd177d62d4: Mounted from deeplearning-platform-release/base-cpu\n",
      "5a26b52752f7: Mounted from deeplearning-platform-release/base-cpu\n",
      "dd4157733cba: Mounted from deeplearning-platform-release/base-cpu\n",
      "e1d00831a5f5: Mounted from deeplearning-platform-release/base-cpu\n",
      "d62cb3f3ee3a: Mounted from deeplearning-platform-release/base-cpu\n",
      "75612067b361: Mounted from deeplearning-platform-release/base-cpu\n",
      "5f70bf18a086: Layer already exists\n",
      "a957f356a963: Mounted from deeplearning-platform-release/base-cpu\n",
      "240c1b858bf3: Mounted from deeplearning-platform-release/base-cpu\n",
      "10cb6c2ac919: Pushed\n",
      "0313db964f3f: Mounted from deeplearning-platform-release/base-cpu\n",
      "ec66d8cea54a: Layer already exists\n",
      "8338899cb13a: Mounted from deeplearning-platform-release/base-cpu\n",
      "4fb340a451dc: Mounted from deeplearning-platform-release/base-cpu\n",
      "e4ea0618e6f3: Mounted from deeplearning-platform-release/base-cpu\n",
      "latest: digest: sha256:0964f8b25c7d3b35addd7ed4801e2bd3f4427dfa96fa62cdccd174e7e7ea3280 size: 5760\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                        STATUS\n",
      "beaba677-5cf1-44f5-b695-9384324c0b79  2023-06-21T08:23:24+00:00  3M59S     gs://qwiklabs-asl-02-3d939c98ad80_cloudbuild/source/1687335803.649876-eda8654d7da645e5b57190653cbf117d.tgz  gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {KFP_CLI_IMAGE_URI} kfp-cli_vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the **Cloud Build** workflow.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "In the cell below, you'll complete the `cloudbuild_vertex.yaml` file describing the CI/CD workflow and prescribing how environment specific settings are abstracted using **Cloud Build** variables.\n",
    "\n",
    "The CI/CD workflow automates the steps you walked through manually during `lab-02_vertex`:\n",
    "1. Builds the trainer image\n",
    "1. Compiles the pipeline\n",
    "1. Uploads and run the pipeline to the Vertex AI Pipeline environment\n",
    "1. Pushes the trainer to your project's **Container Registry**\n",
    " \n",
    "\n",
    "The **Cloud Build** workflow configuration uses both standard and custom [Cloud Build builders](https://cloud.google.com/cloud-build/docs/cloud-builders). The custom builder encapsulates **KFP CLI**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild_vertex.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild_vertex.yaml\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this\n",
    "# file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "    \n",
    "# Unless required by applicable law or agreed to in writing, software \n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either \n",
    "# express or implied. See the License for the specific language governing \n",
    "# permissions and limitations under the License.\n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/trainer_image_covertype_vertex:latest', '.']\n",
    "  dir: $_PIPELINE_FOLDER/trainer_image_vertex\n",
    "  \n",
    "\n",
    "# Compile the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli-vertex'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    dsl-compile-v2 # TODO\n",
    "  env:\n",
    "  - 'PIPELINE_ROOT=gs://$PROJECT_ID-kfp-artifact-store/pipeline'\n",
    "  - 'PROJECT_ID=$PROJECT_ID'\n",
    "  - 'REGION=$_REGION'\n",
    "  - 'SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest'\n",
    "  - 'TRAINING_CONTAINER_IMAGE_URI=gcr.io/$PROJECT_ID/trainer_image_covertype_vertex:latest'\n",
    "  - 'TRAINING_FILE_PATH=gs://$PROJECT_ID-kfp-artifact-store/data/training/dataset.csv'\n",
    "  - 'VALIDATION_FILE_PATH=gs://$PROJECT_ID-kfp-artifact-store/data/validation/dataset.csv'\n",
    "  dir: pipeline_vertex\n",
    "  \n",
    "# Run the pipeline\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli-vertex'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    python kfp-cli_vertex/run_pipeline.py  # TODO\n",
    "    \n",
    "images: [\"gcr.io/$PROJECT_ID/trainer_image_covertype_vertex:latest\"]\n",
    "\n",
    "\n",
    "# This is required since the pipeline run overflows the default timeout\n",
    "timeout: 10800s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually triggering CI/CD runs\n",
    "\n",
    "You can manually trigger **Cloud Build** runs using the [gcloud builds submit command]( https://cloud.google.com/sdk/gcloud/reference/builds/submit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_REGION=us-central1,_PIPELINE_FOLDER=./'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBSTITUTIONS = f\"_REGION={REGION},_PIPELINE_FOLDER=./\"\n",
    "SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 12 file(s) totalling 73.2 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://qwiklabs-asl-02-3d939c98ad80_cloudbuild/source/1687336510.231617-9fb0fbbb98ab4a53911b1749bb5f8553.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-02-3d939c98ad80/locations/global/builds/ecae78c5-d1f1-4153-ad3b-d5a4d0baf0d0].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/ecae78c5-d1f1-4153-ad3b-d5a4d0baf0d0?project=797777062396 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"ecae78c5-d1f1-4153-ad3b-d5a4d0baf0d0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-02-3d939c98ad80_cloudbuild/source/1687336510.231617-9fb0fbbb98ab4a53911b1749bb5f8553.tgz#1687336510524186\n",
      "Copying gs://qwiklabs-asl-02-3d939c98ad80_cloudbuild/source/1687336510.231617-9fb0fbbb98ab4a53911b1749bb5f8553.tgz#1687336510524186...\n",
      "/ [1 files][ 14.9 KiB/ 14.9 KiB]                                                \n",
      "Operation completed over 1 objects/14.9 KiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  6.144kB\n",
      "Step #0: Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #0: latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "Step #0: 56e0351b9876: Pulling fs layer\n",
      "Step #0: 4805448d2887: Pulling fs layer\n",
      "Step #0: 6010ee0be6fa: Pulling fs layer\n",
      "Step #0: 4f4fb700ef54: Pulling fs layer\n",
      "Step #0: e2338d0c32f5: Pulling fs layer\n",
      "Step #0: f44de40d5a81: Pulling fs layer\n",
      "Step #0: 076280f960a0: Pulling fs layer\n",
      "Step #0: 0735612d0a87: Pulling fs layer\n",
      "Step #0: cd1fda9a7709: Pulling fs layer\n",
      "Step #0: 2cace1757ac5: Pulling fs layer\n",
      "Step #0: 9c90296fbff7: Pulling fs layer\n",
      "Step #0: 60fb050d3256: Pulling fs layer\n",
      "Step #0: 49eac6151c74: Pulling fs layer\n",
      "Step #0: 23ce0ff19339: Pulling fs layer\n",
      "Step #0: 2e6deadecd70: Pulling fs layer\n",
      "Step #0: 74755647fd11: Pulling fs layer\n",
      "Step #0: bd19886240a6: Pulling fs layer\n",
      "Step #0: 057629cf5e3d: Pulling fs layer\n",
      "Step #0: ceeeb7fc204d: Pulling fs layer\n",
      "Step #0: 66bc44577f21: Pulling fs layer\n",
      "Step #0: 637548efc9d8: Pulling fs layer\n",
      "Step #0: 4b55ed3e4d80: Pulling fs layer\n",
      "Step #0: 0ec5ac20ec04: Pulling fs layer\n",
      "Step #0: 60fb050d3256: Waiting\n",
      "Step #0: 49eac6151c74: Waiting\n",
      "Step #0: 23ce0ff19339: Waiting\n",
      "Step #0: 2e6deadecd70: Waiting\n",
      "Step #0: 74755647fd11: Waiting\n",
      "Step #0: bd19886240a6: Waiting\n",
      "Step #0: 057629cf5e3d: Waiting\n",
      "Step #0: ceeeb7fc204d: Waiting\n",
      "Step #0: 66bc44577f21: Waiting\n",
      "Step #0: 637548efc9d8: Waiting\n",
      "Step #0: 4b55ed3e4d80: Waiting\n",
      "Step #0: 0ec5ac20ec04: Waiting\n",
      "Step #0: 4f4fb700ef54: Waiting\n",
      "Step #0: e2338d0c32f5: Waiting\n",
      "Step #0: f44de40d5a81: Waiting\n",
      "Step #0: 076280f960a0: Waiting\n",
      "Step #0: 0735612d0a87: Waiting\n",
      "Step #0: cd1fda9a7709: Waiting\n",
      "Step #0: 2cace1757ac5: Waiting\n",
      "Step #0: 9c90296fbff7: Waiting\n",
      "Step #0: 6010ee0be6fa: Verifying Checksum\n",
      "Step #0: 6010ee0be6fa: Download complete\n",
      "Step #0: 4f4fb700ef54: Verifying Checksum\n",
      "Step #0: 4f4fb700ef54: Download complete\n",
      "Step #0: 4805448d2887: Verifying Checksum\n",
      "Step #0: 4805448d2887: Download complete\n",
      "Step #0: 56e0351b9876: Verifying Checksum\n",
      "Step #0: 56e0351b9876: Download complete\n",
      "Step #0: 076280f960a0: Verifying Checksum\n",
      "Step #0: 076280f960a0: Download complete\n",
      "Step #0: 0735612d0a87: Verifying Checksum\n",
      "Step #0: 0735612d0a87: Download complete\n",
      "Step #0: cd1fda9a7709: Verifying Checksum\n",
      "Step #0: cd1fda9a7709: Download complete\n",
      "Step #0: 2cace1757ac5: Verifying Checksum\n",
      "Step #0: 2cace1757ac5: Download complete\n",
      "Step #0: f44de40d5a81: Download complete\n",
      "Step #0: 60fb050d3256: Verifying Checksum\n",
      "Step #0: 60fb050d3256: Download complete\n",
      "Step #0: 49eac6151c74: Verifying Checksum\n",
      "Step #0: 49eac6151c74: Download complete\n",
      "Step #0: 23ce0ff19339: Verifying Checksum\n",
      "Step #0: 23ce0ff19339: Download complete\n",
      "Step #0: 2e6deadecd70: Download complete\n",
      "Step #0: 74755647fd11: Verifying Checksum\n",
      "Step #0: 74755647fd11: Download complete\n",
      "Step #0: bd19886240a6: Verifying Checksum\n",
      "Step #0: bd19886240a6: Download complete\n",
      "Step #0: 057629cf5e3d: Verifying Checksum\n",
      "Step #0: 057629cf5e3d: Download complete\n",
      "Step #0: e2338d0c32f5: Verifying Checksum\n",
      "Step #0: e2338d0c32f5: Download complete\n",
      "Step #0: 66bc44577f21: Verifying Checksum\n",
      "Step #0: 66bc44577f21: Download complete\n",
      "Step #0: ceeeb7fc204d: Verifying Checksum\n",
      "Step #0: ceeeb7fc204d: Download complete\n",
      "Step #0: 637548efc9d8: Verifying Checksum\n",
      "Step #0: 637548efc9d8: Download complete\n",
      "Step #0: 9c90296fbff7: Verifying Checksum\n",
      "Step #0: 9c90296fbff7: Download complete\n",
      "Step #0: 0ec5ac20ec04: Verifying Checksum\n",
      "Step #0: 0ec5ac20ec04: Download complete\n",
      "Step #0: 56e0351b9876: Pull complete\n",
      "Step #0: 4805448d2887: Pull complete\n",
      "Step #0: 6010ee0be6fa: Pull complete\n",
      "Step #0: 4f4fb700ef54: Pull complete\n",
      "Step #0: 4b55ed3e4d80: Verifying Checksum\n",
      "Step #0: 4b55ed3e4d80: Download complete\n",
      "Step #0: e2338d0c32f5: Pull complete\n",
      "Step #0: f44de40d5a81: Pull complete\n",
      "Step #0: 076280f960a0: Pull complete\n",
      "Step #0: 0735612d0a87: Pull complete\n",
      "Step #0: cd1fda9a7709: Pull complete\n",
      "Step #0: 2cace1757ac5: Pull complete\n",
      "Step #0: 9c90296fbff7: Pull complete\n",
      "Step #0: 60fb050d3256: Pull complete\n",
      "Step #0: 49eac6151c74: Pull complete\n",
      "Step #0: 23ce0ff19339: Pull complete\n",
      "Step #0: 2e6deadecd70: Pull complete\n",
      "Step #0: 74755647fd11: Pull complete\n",
      "Step #0: bd19886240a6: Pull complete\n",
      "Step #0: 057629cf5e3d: Pull complete\n",
      "Step #0: ceeeb7fc204d: Pull complete\n",
      "Step #0: 66bc44577f21: Pull complete\n",
      "Step #0: 637548efc9d8: Pull complete\n",
      "Step #0: 4b55ed3e4d80: Pull complete\n",
      "Step #0: 0ec5ac20ec04: Pull complete\n",
      "Step #0: Digest: sha256:cac9f0c3e19cdac79a395ef2ea20db4e36b18e4135a66d02b84dec75f0a3995b\n",
      "Step #0: Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      "Step #0:  ---> 584d1bb363f5\n",
      "Step #0: Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "Step #0:  ---> Running in 27beb770c026\n",
      "Step #0: Collecting fire\n",
      "Step #0:   Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 6.6 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting cloudml-hypertune\n",
      "Step #0:   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting scikit-learn==0.20.4\n",
      "Step #0:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 41.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting pandas==0.24.2\n",
      "Step #0:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 49.2 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.21.6)\n",
      "Step #0: Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Step #0: Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Step #0: Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2023.3)\n",
      "Step #0: Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Step #0: Collecting termcolor (from fire)\n",
      "Step #0:   Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Step #0: Building wheels for collected packages: fire, cloudml-hypertune\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=ad858f955e9befb6fec4db059911819cec99bc93745991fd6acd486b3c095411\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3973 sha256=6b20bdee1c4588e740007f476419791aad1abb41c5883ce56689cd12397bf040\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #0: Successfully built fire cloudml-hypertune\n",
      "Step #0: Installing collected packages: cloudml-hypertune, termcolor, scikit-learn, pandas, fire\n",
      "Step #0:   Attempting uninstall: scikit-learn\n",
      "Step #0:     Found existing installation: scikit-learn 1.0.2\n",
      "Step #0:     Uninstalling scikit-learn-1.0.2:\n",
      "Step #0:       Successfully uninstalled scikit-learn-1.0.2\n",
      "Step #0:   Attempting uninstall: pandas\n",
      "Step #0:     Found existing installation: pandas 1.3.5\n",
      "Step #0:     Uninstalling pandas-1.3.5:\n",
      "Step #0:       Successfully uninstalled pandas-1.3.5\n",
      "Step #0: \u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #0: phik 0.12.3 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: seaborn 0.12.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: statsmodels 0.13.5 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: visions 0.7.5 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: ydata-profiling 4.2.0 requires pandas!=1.4.0,<2,>1.1, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: \u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.5.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-2.3.0\n",
      "Step #0: \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0: \u001b[0mRemoving intermediate container 27beb770c026\n",
      "Step #0:  ---> 5b0d563e8542\n",
      "Step #0: Step 3/5 : WORKDIR /app\n",
      "Step #0:  ---> Running in 2ecb3e889cf6\n",
      "Step #0: Removing intermediate container 2ecb3e889cf6\n",
      "Step #0:  ---> 270f19dc4d0b\n",
      "Step #0: Step 4/5 : COPY train.py .\n",
      "Step #0:  ---> 50ef46286c36\n",
      "Step #0: Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      "Step #0:  ---> Running in 4fe85eda3539\n",
      "Step #0: Removing intermediate container 4fe85eda3539\n",
      "Step #0:  ---> 37026db02e58\n",
      "Step #0: Successfully built 37026db02e58\n",
      "Step #0: Successfully tagged gcr.io/qwiklabs-asl-02-3d939c98ad80/trainer_image_covertype_vertex:latest\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Pulling image: gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex\n",
      "Step #1: Using default tag: latest\n",
      "Step #1: latest: Pulling from qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex\n",
      "Step #1: 56e0351b9876: Already exists\n",
      "Step #1: 4805448d2887: Already exists\n",
      "Step #1: 6010ee0be6fa: Already exists\n",
      "Step #1: 4f4fb700ef54: Already exists\n",
      "Step #1: e2338d0c32f5: Already exists\n",
      "Step #1: f44de40d5a81: Already exists\n",
      "Step #1: 4f4fb700ef54: Already exists\n",
      "Step #1: 076280f960a0: Already exists\n",
      "Step #1: 0735612d0a87: Already exists\n",
      "Step #1: cd1fda9a7709: Already exists\n",
      "Step #1: 2cace1757ac5: Already exists\n",
      "Step #1: 9c90296fbff7: Already exists\n",
      "Step #1: 60fb050d3256: Already exists\n",
      "Step #1: 49eac6151c74: Already exists\n",
      "Step #1: 23ce0ff19339: Already exists\n",
      "Step #1: 2e6deadecd70: Already exists\n",
      "Step #1: 74755647fd11: Already exists\n",
      "Step #1: bd19886240a6: Already exists\n",
      "Step #1: 057629cf5e3d: Already exists\n",
      "Step #1: ceeeb7fc204d: Already exists\n",
      "Step #1: 66bc44577f21: Already exists\n",
      "Step #1: 637548efc9d8: Already exists\n",
      "Step #1: 4b55ed3e4d80: Already exists\n",
      "Step #1: 0ec5ac20ec04: Already exists\n",
      "Step #1: 611722959928: Pulling fs layer\n",
      "Step #1: 817827f0e761: Pulling fs layer\n",
      "Step #1: 817827f0e761: Verifying Checksum\n",
      "Step #1: 817827f0e761: Download complete\n",
      "Step #1: 611722959928: Verifying Checksum\n",
      "Step #1: 611722959928: Download complete\n",
      "Step #1: 611722959928: Pull complete\n",
      "Step #1: 817827f0e761: Pull complete\n",
      "Step #1: Digest: sha256:0964f8b25c7d3b35addd7ed4801e2bd3f4427dfa96fa62cdccd174e7e7ea3280\n",
      "Step #1: Status: Downloaded newer image for gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex:latest\n",
      "Step #1: gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex:latest\n",
      "Step #1: usage: dsl-compile-v2 [-h] --py PY [--function FUNCTION]\n",
      "Step #1:                       [--pipeline-parameters PIPELINE_PARAMETERS]\n",
      "Step #1:                       [--namespace NAMESPACE] --output OUTPUT\n",
      "Step #1:                       [--disable-type-check]\n",
      "Step #1: dsl-compile-v2: error: the following arguments are required: --py, --output\n",
      "Finished Step #1\n",
      "ERROR\n",
      "ERROR: build step 1 \"gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex\" failed: step exited with non-zero status: 2\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "BUILD FAILURE: Build step failure: build step 1 \"gcr.io/qwiklabs-asl-02-3d939c98ad80/kfp-cli-vertex\" failed: step exited with non-zero status: 2\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) build ecae78c5-d1f1-4153-ad3b-d5a4d0baf0d0 completed with status \"FAILURE\"\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . --config cloudbuild_vertex.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you experience issues with CloudBuild being able to access Vertex AI, you may need to run the following commands in **CloudShell**:\n",
    "\n",
    "```\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "PROJECT_NUMBER=$(gcloud projects list --filter=\"name=$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\")\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "  --member serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com \\\n",
    "  --role roles/editor\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com \\\n",
    "    --role roles/storage.objectAdmin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GitHub integration\n",
    "\n",
    "## Exercise\n",
    "\n",
    "In this exercise you integrate your CI/CD workflow with **GitHub**, using [Cloud Build GitHub App](https://github.com/marketplace/google-cloud-build). \n",
    "You will set up a trigger that starts the CI/CD workflow when a new tag is applied to the **GitHub** repo managing the  pipeline source code. You will use a fork of this repo as your source GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a fork of this repo\n",
    "[Follow the GitHub documentation](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) to fork [this repo](https://github.com/GoogleCloudPlatform/asl-ml-immersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reflect yaml file change and create a commit\n",
    "\n",
    "Go to your fork of this repo page, and open `asl-ml-immersion/notebooks/kubeflow_pipelines/cicd/labs/cloudbuild_vertex.yaml` file.\n",
    "\n",
    "Click Edit button and copy your updated yaml file directly to the page.\n",
    "![image](https://user-images.githubusercontent.com/6895245/158727133-e5d77f0c-354c-4b2b-a710-8209ee67571f.png)\n",
    "\n",
    "Click 'Commit changes' button and create a new commit. \n",
    "![image](https://user-images.githubusercontent.com/6895245/158727565-13b4981a-8bce-401b-8f1a-d09a33a163a8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a **Cloud Build** trigger\n",
    "\n",
    "Connect the fork you created in the previous step to your Google Cloud project and create a trigger following the steps in the [Creating GitHub app trigger](https://cloud.google.com/cloud-build/docs/create-github-app-triggers) article. Use the following values on the **Edit trigger** form:\n",
    "\n",
    "|Field|Value|\n",
    "|-----|-----|\n",
    "|Name|[YOUR TRIGGER NAME]|\n",
    "|Description|[YOUR TRIGGER DESCRIPTION]|\n",
    "|Event| Tag|\n",
    "|Source| [YOUR FORK]|\n",
    "|Tag (regex)|.\\*|\n",
    "|Build Configuration|Cloud Build configuration file (yaml or json)|\n",
    "|Cloud Build configuration file location| ./notebooks/kubeflow_pipelines/cicd/labs/cloudbuild_vertex.yaml|\n",
    "\n",
    "\n",
    "Use the following values for the substitution variables:\n",
    "\n",
    "|Variable|Value|\n",
    "|--------|-----|\n",
    "|_REGION|us-central1|\n",
    "|_PIPELINE_FOLDER|notebooks/kubeflow_pipelines/cicd/labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Trigger the build\n",
    "\n",
    "To start an automated build [create a new release of the repo in GitHub](https://help.github.com/en/github/administering-a-repository/creating-releases). Alternatively, you can start the build by applying a tag using `git`. \n",
    "```\n",
    "git tag [TAG NAME]\n",
    "git push origin --tags\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command above, a build should have been automatically triggered, which you should able to inspect [here](https://console.cloud.google.com/cloud-build/builds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
